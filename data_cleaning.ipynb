{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "----\n",
    "In this interactive notebook, we explore the process of cleaning up daily summary data downloaded en masse through the AQS website. The website provides ZIP archives of CSV files for each of the parameters of Criteria gasses, which are all contained in the AQI defined pollutants class of parameters. I have collected ZIP files with daily summary data for each of Carbon Monoxide (CO), Ozone (O3), Sulfur Dioxide (SO2), and Nitrogen Dioxide (NO2) from 2010 to 2018.\n",
    "\n",
    "Let's start by importing our required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyaqs import AQSFetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's instantiate an object that will handle fetching some parameter and state codes from the AQS API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqs_fetcher = AQSFetcher('bbjornstad.flatiron@gmail.com', 'ochrefox21')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can use this object to get our needed codes. We will also save these into the `cleaned_data` folder so that they can also be easily accessed for annotation and iteration purposes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_codes = aqs_fetcher.get_parameter_list_by_class('AQI POLLUTANTS')\n",
    "aqi_gasses = param_codes.iloc[:4,:]\n",
    "aqi_gasses['param_shorthand'] = ['CO', 'SO2', 'NO2', 'Ozone']\n",
    "aqi_gasses['legend_entry'] = aqi_gasses.apply(lambda r: f'{r.code} - {r.param_shorthand}', axis=1)\n",
    "\n",
    "state_codes = aqs_fetcher.get_state_codes()\n",
    "# filter out some junk locations that don't have full data\n",
    "state_codes = state_codes.loc[state_codes.code != 'CC']\n",
    "state_codes.code = state_codes.code.astype(int)\n",
    "state_codes = state_codes.loc[state_codes.code < 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_gasses.to_csv('cleaned_data/aqi_gasses.csv', index=False)\n",
    "state_codes.to_csv('cleaned_data/state_codes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a variable to hold the root location of all of our air quality data sources for our selected gasses. These files are stored in ZIP format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'raw_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store a list holding all of the years for which we have downloaded daily summary data for each of the above four gas parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this, we can import and clean up our datasets. Let's create a master list of unneeded columns, so that we can easily drop them from each imported dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'POC',\n",
    "    'Latitude',\n",
    "    'Longitude',\n",
    "    'Datum',\n",
    "    'Pollutant Standard',\n",
    "    'Event Type',\n",
    "    'Observation Count',\n",
    "    'Observation Percent',\n",
    "    '1st Max Value',\n",
    "    '1st Max Hour',\n",
    "    'Method Code',\n",
    "    'Method Name',\n",
    "    'Address',\n",
    "    'CBSA Name',\n",
    "    'Date of Last Change',\n",
    "    'Local Site Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that our data is handled in the same fashion for each file, we should probably write a function that can handle importation of a particular parameter code for a particular year. This corresponds to one particular file within the `raw_data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_aggregate_data(param_code, year):\n",
    "    path_stub = f'/daily_{param_code}_{year}.zip'\n",
    "    path = data_folder+path_stub\n",
    "    df = pd.read_csv(path)\n",
    "    df.drop(columns = cols_to_drop, inplace=True)\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    df.date_local = pd.to_datetime(df.date_local)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can import our dataframes for each gas to create a master dataframe, on which we will perform some aggregation to make our data formatted appropriately for our future modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42101\n",
      "42401\n",
      "42602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44201\n"
     ]
    }
   ],
   "source": [
    "all_data_raw_agg_trimmed = pd.DataFrame()\n",
    "\n",
    "for param_code in aqi_gasses.code:\n",
    "    all_param_df = pd.DataFrame()\n",
    "    for year in date_range:\n",
    "        yearly_param_df = import_aggregate_data(param_code, year)\n",
    "        all_param_df = pd.concat([all_param_df, yearly_param_df])\n",
    "        \n",
    "    all_param_df.to_csv(f'intermediate_data/{param_code}_daily.csv')\n",
    "    \n",
    "    all_data_raw_agg_trimmed = pd.concat([all_data_raw_agg_trimmed, all_param_df])\n",
    "    print(f'Imported {param_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importation, we should check some basic descriptive information provided by Pandas to ensure that everything has been appropriately imported and is ready for formatting and further filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9710154, 13)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_raw_agg_trimmed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9710154 entries, 0 to 402633\n",
      "Data columns (total 13 columns):\n",
      "state_code          int64\n",
      "county_code         int64\n",
      "site_num            int64\n",
      "parameter_code      int64\n",
      "parameter_name      object\n",
      "sample_duration     object\n",
      "date_local          datetime64[ns]\n",
      "units_of_measure    object\n",
      "arithmetic_mean     float64\n",
      "aqi                 float64\n",
      "state_name          object\n",
      "county_name         object\n",
      "city_name           object\n",
      "dtypes: datetime64[ns](1), float64(2), int64(4), object(6)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "all_data_raw_agg_trimmed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are most interested in the `aqi` column, which represents the calculated Air Quality Index per the EPA standards of calculation, which is well defined for each of the four selected gasses. However, not all of the AQI entries are populated appropriately. Because of the massive collection of data that we have managed to access, we can safely simply drop these entries from our dataframe to ensure that we have all non-null data for future modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_no_nan_aqi = all_data_raw_agg_trimmed.dropna(subset=['aqi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7262623 entries, 355 to 402633\n",
      "Data columns (total 13 columns):\n",
      "state_code          int64\n",
      "county_code         int64\n",
      "site_num            int64\n",
      "parameter_code      int64\n",
      "parameter_name      object\n",
      "sample_duration     object\n",
      "date_local          datetime64[ns]\n",
      "units_of_measure    object\n",
      "arithmetic_mean     float64\n",
      "aqi                 float64\n",
      "state_name          object\n",
      "county_name         object\n",
      "city_name           object\n",
      "dtypes: datetime64[ns](1), float64(2), int64(4), object(6)\n",
      "memory usage: 775.7+ MB\n"
     ]
    }
   ],
   "source": [
    "all_data_no_nan_aqi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016-05-26    2609\n",
       "2016-07-22    2584\n",
       "2016-07-20    2584\n",
       "2016-05-24    2577\n",
       "2016-05-23    2548\n",
       "              ... \n",
       "2010-11-28    1738\n",
       "2010-12-24    1737\n",
       "2010-12-27    1734\n",
       "2010-12-25    1734\n",
       "2010-12-26    1729\n",
       "Name: date_local, Length: 3287, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_no_nan_aqi.date_local.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-HR RUN AVG BEGIN HOUR    3502101\n",
       "1 HOUR                     2793432\n",
       "8-HR RUN AVG END HOUR       967090\n",
       "Name: sample_duration, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_no_nan_aqi.sample_duration.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_no_nan_aqi = all_data_no_nan_aqi.loc[all_data_no_nan_aqi.state_name.isin(state_codes.state_name)]\n",
    "len(all_data_no_nan_aqi.state_name.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have appropriately filtered the data, and ensured that all entries for the Air Quality Index are non-null, we aim to take an average AQI reading for each state on each day for each of the specified four parameters. This is easily accomplished with a Pandas groupby statement. After grouping, we will have a few superfluous columns whose meaning will have been lost, and we can safely remove these from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_aggregation_no_nan = all_data_no_nan_aqi.groupby(['state_name', 'parameter_code', 'date_local']).mean()\n",
    "mean_aggregation_no_nan = mean_aggregation_no_nan.drop(columns=['county_code', 'site_num', 'state_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>arithmetic_mean</th>\n",
       "      <th>aqi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_name</th>\n",
       "      <th>parameter_code</th>\n",
       "      <th>date_local</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Alabama</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">42101</th>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>0.280702</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>0.288889</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.361111</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.404166</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.413889</td>\n",
       "      <td>10.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.719444</td>\n",
       "      <td>14.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.306944</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-09</th>\n",
       "      <td>0.283333</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-10</th>\n",
       "      <td>0.329167</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      arithmetic_mean        aqi\n",
       "state_name parameter_code date_local                            \n",
       "Alabama    42101          2010-01-01         0.280702   3.333333\n",
       "                          2010-01-02         0.288889   3.333333\n",
       "                          2010-01-03         0.277778   3.333333\n",
       "                          2010-01-04         0.361111   5.333333\n",
       "                          2010-01-05         0.404166   5.333333\n",
       "                          2010-01-06         0.413889  10.666667\n",
       "                          2010-01-07         0.719444  14.333333\n",
       "                          2010-01-08         0.306944   4.333333\n",
       "                          2010-01-09         0.283333   4.000000\n",
       "                          2010-01-10         0.329167   6.666667"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_aggregation_no_nan.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save our cleaned and aggregated data to our `cleaned_data` folder, so that we can easily access the dataset in future modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_aggregation_no_nan.to_csv('cleaned_data/criteria_gasses_aqi_means.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
